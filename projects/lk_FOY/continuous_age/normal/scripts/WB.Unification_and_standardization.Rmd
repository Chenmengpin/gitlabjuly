---
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: journal
    css: custom.css
title: "Unification and Standardization of WB Studies"
---

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(warning=FALSE, error=FALSE, message=FALSE, echo=FALSE, cache=FALSE, tidy.opts=list(keep.blank.line=FALSE, width.cutoff=120), dev="svg")
options(width=200)
```

# UNIFICATION AND STANDARDIZATION OF STUDIES

---

Analysis of public datasets for differential gene expression signatures as an outcome of age for [Les Kobzik](mailto:LKOBZIK@hsph.harvard.edu) as part of the DARPA 7-day Biodefense Program. Whole blood samples only.

Contact [John Hutchinson](mailto:jhutchin@hsph.harvard.edu) for additional details.

The most recent update of this html document occurred: `r date()`

The sections below provide code to reproduce the included results and plots. 

----

# GENERAL SETUP

## General purpose R libraries necessary for analysis

```{r general_libraries}
source("http://bioconductor.org/biocLite.R") # BioConductor script necessary for installing new BioC libraries with biocLite()
library(xtable) # table generation for reports
library(plyr) # library for iteratively working with data structures
library(ggplot2) # library for plotting 
library(RColorBrewer) # library for generating color palettes
library(googleVis) # library for presenting tables
```

## Locations of directories for data, metadata and results

```{r general_directories}
if (file.exists("/n/hsphS10/hsphfs1/chb/projects/lk_FOY/continuous_age/normal")) {
  baseDir <- "/n/hsphS10/hsphfs1/chb/projects/lk_FOY/continuous_age/normal"
  }  else if (file.exists("/Volumes/home08/jhutchin/consults/lk_FOY/continuous_age/normal")) {
    baseDir <- "/Volumes/home08/jhutchin/consults/lk_FOY/continuous_age/normal"
    } else {
      baseDir <- "/Volumes/ody/consults/lk_FOY/continuous_age/normal"
      }
dataDir <- file.path(baseDir, "data")
resultsDir <- file.path(baseDir, "results", "WB")
metaDir <- file.path(baseDir, "meta", "WB")
````

----

# INITIAL DOWNLOAD of METATDATA
## Obtaining metadata for all studies 
In this section, I downloaded the metadata for all the whole blood studies Les shared in his [file](../meta/WB/UpdatedDatasetWishListFeb2015.xlsx)
I used two [Bioconductor][1] libraries to interface with the [Gene Expression Omnibus][2] and [Array Express][3] databases ([GEOquery][4] and [ArrayExpress][5] respectively). 

```{r database_download_libraries}
library(GEOquery)
library(ArrayExpress)
```

### Load in the series IDs of the GEO studies

The IDs of the GEO series (GSEs) are:

```{r GEOIDs}
GSE.ids <- c("GSE10715", "GSE11375", "GSE16028", "GSE16059", "GSE18897", "GSE19743", "GSE22868", "GSE3284","GSE37171", "GSE14844", "GSE26378", "GSE26440", "GSE8121", "GSE9692", "GSE35571", "GSE46449", "GSE16059", "GSE25414",  "GSE16028", "GSE29691", "GSE48060", "GSE37667")
GSE.ids=sub(" .*" , "", GSE.ids)
GSE.ids=sub(" .*" , "", GSE.ids)
print(GSE.ids)
```

**There are `r length(GSE.ids)` GEO series in total**

### Data for each GEO series was downloaded via GEOquery into separate directories by series ID
- for series with more than one dataset (GDS), I numbered the metadata files separately, according to their order in the series  metadata
- for series with only one dataset, the metadata file contains the number "1"

```{r GEOquerypDATA, eval=FALSE}
for (GSE.id in GSE.ids) {
  # name of the directory receiveing the files
  gse.dataDir=file.path(dataDir, GSE.id) 
  # check to see if this directory exists already 
  if (!(file.exists(gse.dataDir))) { 
    # if it does not, create it
    dir.create(gse.dataDir) 
    }
  # pull down the available series data, using exception handling (i.e. try())
  gse <- try(getGEO(GSE.id, destdir=gse.dataDir)) 
  # for every dataset within the series
  for(n in 1:length(gse)){ 
    # grab the metadata
    metadata <- pData(gse[[n]]) 
     # discard columns you aren't interested in
    metadata <- metadata[,!grepl("data_row_count|status|submission_date|last_update_date|channel_count|scan_protocol|data_processing|hyb_protocol|taxid_ch1|label|contact", names(metadata))]
    # filename to writout the metadata, contains the series id and the number of the dataset 
    file.out <- paste("metadata", GSE.id, n, "tab", sep=".") 
    # write out to tab-delimited file, retaining column names
    write.table(metadata, file=file.path(gse.dataDir, file.out), quote=F, sep="\t", col.names=T, row.names=F) 
    # sleep 5 seconds between downloads so you don't hammer the GEO server, as it seems to be a bit unstable
    Sys.sleep(5) 
    }
  }
```


### Load in the Array Express IDs of the Array Express studies
The IDs of the Array Express studies are: 

```{r AEids}
AE.ids <- c("MEXP-2917", "MEXP-3567", "MEXP-884", "MTAB-25", "TABM-666","TABM-940")
print(AE.ids)
```

### Data for each Array Express series was downloaded with ArrayExpress into separate directories by series ID

```{r AEquerypData, eval=FALSE}
for (AE.id in AE.ids) {
  ae.dataDir=file.path(dataDir, AE.id)
  if (!(file.exists(ae.dataDir))) {
    dir.create(ae.dataDir)
  }
  ae <-   try(ArrayExpress(accession=paste("E", AE.id, sep="-"), path=ae.dataDir, save=TRUE,  ))
  Sys.sleep(5)
}
```

- for the Array Express studies, there was no need to export the metadata from R as the ArrayExpress library downloads all raw data files and metadata
  - metadata was saved in *sdrf files
- edited all the Array Express metadata files manually for import into R
  - removed spaces from headers
  - converted all "#" characters to "_" characters in the main body of the table
  - reformatted line breaks where necessary

These GEO and ArrayExpress metadata files were then individually hand examined to determine, if available:  
1. column headers for:
- unique sample identifier (for GSE studies, this is the GSM id, for ArrayExpress studies, the uniquely assigned study id)
- sample type (multiple if necessary)
- age
- gender
- ethnicity
- FTP location for raw data
- microarray platform
- series ID
- database
2. regular expressions to identify:
- the raw data file
- the control samples

To allow later extraction of the relevant subset of each study's metadata, these values were then compiled in a [columnID translator table](../meta/WB/columnid.translator.tab).

## Preparing Curated Metadata from Les
Les provided an Excel file with manually curated metadata for 24 studies, each with its own worksheet within the file.  
This file was manually modified to make a [new file](../meta/WB/MasterOneAllWholeBloodU133PlusFeb22.2015-JHUpdate.xlsx)  with:  
1. a column with the study ID in every sheet
2. consistent columns headers between sheets

Each curated study's sheet of metadata from this new Excel file was extracted into a table in the study's directory for later import into R.

```{r extract_curated_metadata, eval=FALSE}
library(gdata)
for (sheetnum in 1:25){
  print(sheetnum)
  metadata <- read.xls(file.path(metaDir, "MasterOneAllWholeBloodU133PlusFeb22.2015-JHUpdate.xlsx"), sheet=sheetnum)
  study <- unique(metadata$study)
  write.table(metadata, file=file.path(dataDir, study, paste("curated.metadata", study, "tab", sep=".")), row.names=F, col.names=T, sep="\t", quote=F)
} 
```

## Unifying the metadata into a single file
The goal here was to obtain a metadata file that combined the downloaded metadata and the curated metadata, using the "columnID translator"" as a guide. 

```{r unify_metadata}
## load in the columnID translator
template <- read.table(file.path(metaDir, "columnid.translator.tab"), header=T, colClasses="character", sep="\t") 
## setup list to receive results, each study will be an element in the list
output.l <- list() 
for(rownum in 1:nrow(template)) {
  colids <- template[rownum,] 
  study <- colids$study
  print(study)
  print(rownum)
  list.index <- colids$list.index
  database <- colids$Database
  study.metadata <- read.delim(file.path(dataDir, study, paste("metadata", study, list.index, "tab", sep="." )), header=T, sep="\t") # pull in the study metadata for the local directory
  ## get sampleIDs
  if (is.na(colids$sampleID)) {sampleIDs <- rep(NA, nrow(study.metadata))} else { sampleIDs <- study.metadata[,colids$sampleID]} # if there are no sampleIDs, fill with NA
  ## get sampletypes
  if(is.na(colids$sampltype_col.1)){
     ## if there is no sampletype column #1, fill with NA's
    sampletypes=rep("control", nrow(study.metadata))
  } else {
    # if there is no sampletype column #2, but there is a sampletype column #1, take values from sampletype column #1
    if(is.na(colids$sampletype_col.2)){ 
      sampletypes <- study.metadata[,colids$sampltype_col.1]
    } else {
      # if there are two columns that describe the sampletype, grab them both and paste them together
      sampletypes <- paste(study.metadata[,colids$sampltype_col.1], study.metadata[,colids$sampletype_col.2]) 
    }
  }
  # get ages, fill with NA if no column identified
  if (is.na(colids$age_col)) {ages <- rep(NA, nrow(study.metadata))} else { ages <- study.metadata[,colids$age_col]} 
  # get genders, fill with NA if no column identified   
  if (is.na(colids$gender_col)) {genders <- rep(NA, nrow(study.metadata))} else { genders <- study.metadata[,colids$gender_col]}
  # get ethnicities, fill with NA if no column identified   
  if (is.na(colids$ethnicity_col)) {ethnicities <- rep(NA, nrow(study.metadata))} else { ethnicities <- study.metadata[,colids$ethnicity_col]}
  # get FTP locations of cel files, fill with NA if no column identified   
  if (is.na(colids$CEL_FTP_col)) {CEL_FTPs <- rep(NA, nrow(study.metadata))} else { CEL_FTPs <- study.metadata[,colids$CEL_FTP_col]}
  # get CEL regex identifiers to enable pulling from local directories, fill with NA if no column identified   
  if (is.na(colids$CEL_regex_col)) {CEL_regexes <- rep(NA, nrow(study.metadata))} else { CEL_regexes <- study.metadata[,colids$CEL_regex_col]}
  # get platformid, fill with NA if no column identified   
  if (is.na(colids$platformid_col)) {platformids <- rep(NA, nrow(study.metadata))} else { platformids <- study.metadata[,colids$platformid_col]}
  # make dataframe
  sample.metadata.sub <- as.data.frame(cbind(as.character(sampleIDs), as.character(sampletypes), as.character(ages), as.character(genders), as.character(ethnicities), as.character(CEL_FTPs), as.character(CEL_regexes), as.character(platformids)))
  names(sample.metadata.sub) <- c("sampleID", "sampletype", "age", "gender", "ethnicity", "CEL_FTP", "CEL_regex", "platform")
  sample.metadata.sub$study <- study # fill in columns with study ID and database ID
  sample.metadata.sub$database <- database
  # subset to the control samples using the control sampletype regex for that study
  # adjust for cases where multiple regexes are necessary to identify a control sample
  if (is.na(colids$control_regex.2) & !is.na(colids$control_regex.1)) { 
    sample.metadata.sub <- sample.metadata.sub[grep(colids$control_regex.1, sample.metadata.sub$sampletype),]
  } else if (!is.na(colids$control_regex.2) & !is.na(colids$control_regex.1)){
    sample.metadata.sub <- sample.metadata.sub[intersect(grep(colids$control_regex.1, sample.metadata.sub$sampletype), grep(colids$control_regex.2, sample.metadata.sub$sampletype)),]
  }
  # if curated data exists for this study, replace what you downloaded from GEO/ArrayExpress with his curated data
  if(file.exists(file.path(dataDir, study, paste("curated.metadata", study, "tab", sep=".")))){
    curated.metadata <- read.table(file.path(dataDir, study, paste("curated.metadata", study, "tab", sep=".")), sep="\t", header=T)
    # find the columns you are going to replace in the original downloaded metadata
    replace.cols <- setdiff(names(sample.metadata.sub)[names(sample.metadata.sub) %in% names(curated.metadata)], c("sampleID", "study", "database"))    
    for (replace.col in replace.cols){
      # convert replacement column to characters, otherwise error
      sample.metadata.sub[,replace.col] <- as.character(sample.metadata.sub[,replace.col])
      # subset the curation data dataframe to relevant ids 
      curated.metadata <- curated.metadata[(curated.metadata$sampleID %in% sample.metadata.sub$sampleID),]
      # replace the columns in the original metadata dataframe with the sampleID matched curated metadata
      sample.metadata.sub[,replace.col][na.omit(match(curated.metadata$sampleID,sample.metadata.sub$sampleID))] <- as.character(curated.metadata[,replace.col])
    }
  }
  # output the control sample metadata for the study
  output.l[[rownum]] <- sample.metadata.sub 
}
# collapse the metadata list into a dataframe
output <- do.call(rbind, output.l) 
# output to file
write.table(output, file.path(metaDir, "unified.metadata.unrefined.tab"), quote=F, col.names=T, row.names=F, sep="\t")
```

The file with the unified (but not consistent) metadata can be found [here](../meta/WB/unified.metadata.unrefined.tab)
*(this is a tab-delimited file that can be opened in Excel)*
The end goal is a file containing the metadata for all control samples in all studies. To generate a file that can be then be used to load in the metadata and its respective raw data later in the analysis, I also needed consistent labeling of gender, age, ethnicity, FTP location, and CEL file search terms etc. 

To make this file, I ran the "unified.metadata.unrefined.tab" file through [Google refine][6], merging multiple terms for the same thing into a single term. For example, terms such as "F", "Female", "Fem", and "f" were merged into the single term FEMALE. All ages were converted to years based units. For ethnicity, African Origin encompasses the terms: "African American, Afro-American, Black, African etc.". Asian encompasses the terms: "Chinese, Asian, Indian and Oriental". Caucasian encompasses the terms: " White and Caucasian".

The unified, refined metadata file can be found [here](../meta/WB/unified.metadata.refined.tab)

---

A separate python script was used to download all the GEO CEL files (using their ftp locations from the metadata file). Array Express CEL files were previously automatically downloaded alongside their metadata. 


[1]: http://www.bioconductor.org (BioC)
[2]: http://www.ncbi.nlm.nih.gov/gds/ (GEO)
[3]: http://www.ebi.ac.uk/arrayexpress/ (ArrayExpress)
[4]: http://www.bioconductor.org/packages/2.11/bioc/html/GEOquery.html (GEOquery_BioC_library)
[5]: http://www.bioconductor.org/packages/2.11/bioc/html/ArrayExpress.html (ArrayExpress_BioC_library)
[6]: https://code.google.com/p/google-refine/ (Google_Refine)


